{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tridiagonal matrix solver benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=1\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# %env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  5 18:03:43 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 207...  Off  | 00000000:09:00.0  On |                  N/A |\n",
      "|  0%   36C    P8    12W / 215W |    456MiB /  7974MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1208      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A      2267      G   /usr/lib/xorg/Xorg                126MiB |\n",
      "|    0   N/A  N/A      2397      G   /usr/bin/gnome-shell              141MiB |\n",
      "|    0   N/A  N/A      2713      G   /usr/lib/firefox/firefox            2MiB |\n",
      "|    0   N/A  N/A      8613      G   ...AAAAAAAAA= --shared-files      134MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load libllvmlite.so .. os.environ[PATH] is: /home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/till/anaconda3/envs/pyhpc-bench-gpu/bin:/home/till/anaconda3/condabin:/usr/local/cuda-11.0/bin:/home/till/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "Checking /home/till/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/llvmlite/binding/libllvmlite.so\n",
      "load_library_permanently(libsvml.so)\n",
      "load_library_permanently .. so.environ[PATH] is: /home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/till/anaconda3/envs/pyhpc-bench-gpu/bin:/home/till/anaconda3/condabin:/usr/local/cuda-11.0/bin:/home/till/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "load_library_permanently(libsvml.so)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import numba.cuda\n",
    "from tke import tke\n",
    "from scipy.linalg import lapack\n",
    "from jax_xla import tridiag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (360, 160, 115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement TDMA\n",
    "\n",
    "#### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_naive(a, b, c, d):\n",
    "    \"\"\"\n",
    "    Solves many tridiagonal matrix systems with diagonals a, b, c and RHS vectors d.\n",
    "    \"\"\"\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    n = a.shape[-1]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        w = a[..., i] / b[..., i - 1]\n",
    "        b[..., i] += -w * c[..., i - 1]\n",
    "        d[..., i] += -w * d[..., i - 1]\n",
    "\n",
    "    out = np.empty_like(a)\n",
    "    out[..., -1] = d[..., -1] / b[..., -1]\n",
    "\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        out[..., i] = (d[..., i] - c[..., i] * out[..., i + 1]) / b[..., i]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_lapack(a, b, c, d):\n",
    "    a[..., 0] = c[..., -1] = 0  # remove couplings between slices\n",
    "    return lapack.dgtsv(a.flatten()[1:], b.flatten(), c.flatten()[:-1], d.flatten())[3].reshape(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.guvectorize([(nb.float64[:],) * 5], '(n), (n), (n), (n) -> (n)', nopython=True)\n",
    "def tdma_numba(a, b, c, d, out):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    n = a.shape[0]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        w = a[i] / b[i - 1]\n",
    "        b[i] += -w * c[i - 1]\n",
    "        d[i] += -w * d[i - 1]\n",
    "\n",
    "    out[-1] = d[-1] / b[-1]\n",
    "\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        out[i] = (d[i] - c[i] * out[i + 1]) / b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nconst = shape[-1]\n",
    "\n",
    "\n",
    "@nb.cuda.jit()\n",
    "def tdma_numba_cuda_kernel(a, b, c, d, out):\n",
    "    i, j = nb.cuda.grid(2)\n",
    "    \n",
    "    if not(i < a.shape[0] and j < a.shape[1]):\n",
    "        return\n",
    "\n",
    "    n = a.shape[2]\n",
    "    \n",
    "    cp = nb.cuda.local.array((nconst,), dtype=nb.float64)\n",
    "    dp = nb.cuda.local.array((nconst,), dtype=nb.float64)\n",
    "    \n",
    "    cp[0] = c[i, j, 0] / b[i, j, 0]\n",
    "    dp[0] = d[i, j, 0] / b[i, j, 0]\n",
    "    \n",
    "    for k in range(1, n):\n",
    "        norm_factor = b[i, j, k] - a[i, j, k] * cp[k-1]\n",
    "        cp[k] = c[i, j, k] / norm_factor\n",
    "        dp[k] = (d[i, j, k] - a[i, j, k] * dp[k-1]) / norm_factor\n",
    "\n",
    "    out[i, j, n-1] = dp[n-1]\n",
    "\n",
    "    for k in range(n - 2, -1, -1):\n",
    "        out[i, j, k] = dp[k] - cp[k] * out[i, j, k+1]\n",
    "        \n",
    "\n",
    "def tdma_numba_cuda(a, b, c, d):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    threadsperblock = (16, 16)\n",
    "    blockspergrid_x = math.ceil(a.shape[0] / threadsperblock[0])\n",
    "    blockspergrid_y = math.ceil(a.shape[1] / threadsperblock[1])\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    out = nb.cuda.device_array(a.shape, dtype=a.dtype)\n",
    "    tdma_numba_cuda_kernel[blockspergrid, threadsperblock](a, b, c, d, out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.lax\n",
    "\n",
    "\n",
    "def tdma_jax_kernel(a, b, c, d):\n",
    "    def compute_primes(last_primes, x):\n",
    "        last_cp, last_dp = last_primes\n",
    "        a, b, c, d = x\n",
    "\n",
    "        denom = 1. / (b - a * last_cp)\n",
    "        cp = c * denom\n",
    "        dp = (d - a * last_dp) * denom\n",
    "\n",
    "        new_primes = (cp, dp)\n",
    "        return new_primes, new_primes\n",
    "\n",
    "    diags = (a.T, b.T, c.T, d.T)\n",
    "    init = jnp.zeros((a.shape[1], a.shape[0]))\n",
    "    _, (cp, dp) = jax.lax.scan(compute_primes, (init, init), diags)\n",
    "\n",
    "    def backsubstitution(last_x, x):\n",
    "        cp, dp = x\n",
    "        new_x = dp - cp * last_x\n",
    "        return new_x, new_x\n",
    "\n",
    "    _, sol = jax.lax.scan(backsubstitution, init, (cp[::-1], dp[::-1]))\n",
    "\n",
    "    return sol[::-1].T\n",
    "\n",
    "\n",
    "tdma_jax = jax.jit(tdma_jax_kernel, backend='cpu')\n",
    "tdma_jax_cuda = jax.jit(tridiag.tridiag, backend='gpu') # jax.jit(tdma_jax_kernel, backend='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "kernel_old = Template('''\n",
    "extern \"C\" __global__\n",
    "void execute(\n",
    "    const ${DTYPE} *a,\n",
    "    const ${DTYPE} *b,\n",
    "    const ${DTYPE} *c,\n",
    "    const ${DTYPE} *d,\n",
    "    ${DTYPE} *solution\n",
    "){\n",
    "    const size_t m = ${SYS_DEPTH};\n",
    "    const size_t total_size = ${SIZE};\n",
    "    const size_t idx = (blockIdx.x * blockDim.x + threadIdx.x) * m;\n",
    "\n",
    "    if (idx >= total_size) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    ${DTYPE} cp[${SYS_DEPTH}];\n",
    "    ${DTYPE} dp[${SYS_DEPTH}];\n",
    "\n",
    "    cp[0] = c[idx] / b[idx];\n",
    "    dp[0] = d[idx] / b[idx];\n",
    "\n",
    "    for (ptrdiff_t j = 1; j < m; ++j) {\n",
    "        const ${DTYPE} norm_factor = b[idx+j] - a[idx+j] * cp[j-1];\n",
    "        cp[j] = c[idx+j] / norm_factor;\n",
    "        dp[j] = (d[idx+j] - a[idx+j] * dp[j-1]) / norm_factor;\n",
    "    }\n",
    "\n",
    "    solution[idx + m-1] = dp[m-1];\n",
    "    for (ptrdiff_t j=m-2; j >= 0; --j) {\n",
    "        solution[idx + j] = dp[j] - cp[j] * solution[idx + j+1];\n",
    "    }\n",
    "}\n",
    "''').substitute(\n",
    "    DTYPE='double',\n",
    "    SYS_DEPTH=shape[-1],\n",
    "    SIZE=np.product(shape)\n",
    ")\n",
    "\n",
    "tdma_cupy_kernel_old = cupy.RawKernel(kernel_old, 'execute')\n",
    "\n",
    "\n",
    "def tdma_cupy_old(a, b, c, d,  blocksize=256):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "    a, b, c, d = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "    out = cupy.empty(a.shape, dtype=a.dtype)\n",
    "    \n",
    "    tdma_cupy_kernel_old(\n",
    "        (math.ceil(a.size / a.shape[-1] / blocksize),),\n",
    "        (blocksize,),\n",
    "        (a, b, c, d, out)\n",
    "    )\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_dim = 32\n",
    "block_rows = 8 \n",
    "transpose4_kernel = Template('''\n",
    "extern \"C\" __global__\n",
    "    void transpose4(\n",
    "  const ${DTYPE}* a,\n",
    "  const ${DTYPE}* b,\n",
    "  const ${DTYPE}* c,\n",
    "  const ${DTYPE}* d,\n",
    "  ${DTYPE}* a_t,\n",
    "  ${DTYPE}* b_t,\n",
    "  ${DTYPE}* c_t,\n",
    "  ${DTYPE}* d_t,\n",
    "  int xdim,\n",
    "  int ydim,\n",
    "  int total_size\n",
    ")\n",
    "{\n",
    "  __shared__ ${DTYPE} tile[4*${TILE_DIM}][${TILE_DIM}+1];\n",
    "\n",
    "  int x = blockIdx.x * ${TILE_DIM} + threadIdx.x;\n",
    "  int y = blockIdx.y * ${TILE_DIM} + threadIdx.y;\n",
    "  \n",
    "  if (x < xdim)\n",
    "  {\n",
    "    for (int j = 0; j < ${TILE_DIM}; j += ${BLOCK_ROWS})\n",
    "    {\n",
    "        int index = (y+j)*xdim + x;\n",
    "        if (index < total_size)\n",
    "        {  \n",
    "          tile[threadIdx.y+j][threadIdx.x] = a[index];\n",
    "          tile[${TILE_DIM} + threadIdx.y+j][threadIdx.x] = b[index];\n",
    "          tile[2 * ${TILE_DIM} + threadIdx.y+j][threadIdx.x] = c[index];\n",
    "          tile[3 * ${TILE_DIM} + threadIdx.y+j][threadIdx.x] = d[index];\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  x = blockIdx.y * ${TILE_DIM} + threadIdx.x;  // transpose block offset\n",
    "  y = blockIdx.x * ${TILE_DIM} + threadIdx.y;\n",
    "  if (x < ydim)\n",
    "  {\n",
    "    for (int j = 0; j < ${TILE_DIM}; j += ${BLOCK_ROWS})\n",
    "    {\n",
    "      int index = (y+j)*ydim + x;\n",
    "      if (index < total_size)\n",
    "      {\n",
    "        \n",
    "        a_t[index] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        b_t[index] = tile[${TILE_DIM} + threadIdx.x][threadIdx.y + j];\n",
    "        c_t[index] = tile[2 * ${TILE_DIM} + threadIdx.x][threadIdx.y + j];\n",
    "        d_t[index] = tile[3 * ${TILE_DIM} + threadIdx.x][threadIdx.y + j];\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''').substitute(\n",
    "        DTYPE='double',\n",
    "        TILE_DIM=tile_dim,\n",
    "        BLOCK_ROWS=block_rows\n",
    ")\n",
    "\n",
    "\n",
    "transpose_kernel = Template('''\n",
    "extern \"C\" __global__\n",
    "void transpose(\n",
    "  const ${DTYPE}* m,\n",
    "  ${DTYPE}* m_t,\n",
    "  int xdim,\n",
    "  int ydim,\n",
    "  int total_size\n",
    ")\n",
    "{\n",
    "  __shared__ ${DTYPE} tile[${TILE_DIM}][${TILE_DIM}+1];\n",
    "\n",
    "  int x = blockIdx.x * ${TILE_DIM} + threadIdx.x;\n",
    "  int y = blockIdx.y * ${TILE_DIM} + threadIdx.y;\n",
    "  \n",
    "  if (x < xdim)\n",
    "  {\n",
    "    for (int j = 0; j < ${TILE_DIM}; j += ${BLOCK_ROWS})\n",
    "    {\n",
    "        int index = (y+j)*xdim + x;\n",
    "        if (index < total_size)\n",
    "          tile[threadIdx.y+j][threadIdx.x] = m[index];\n",
    "    }\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  x = blockIdx.y * ${TILE_DIM} + threadIdx.x;  // transpose block offset\n",
    "  y = blockIdx.x * ${TILE_DIM} + threadIdx.y;\n",
    "  if (x < ydim)\n",
    "  {\n",
    "    for (int j = 0; j < ${TILE_DIM}; j += ${BLOCK_ROWS})\n",
    "    {\n",
    "      int index = (y+j)*ydim + x;\n",
    "      if (index < total_size)\n",
    "        m_t[index] = tile[threadIdx.x][threadIdx.y + j];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''').substitute(\n",
    "        DTYPE='double',\n",
    "        TILE_DIM=tile_dim,\n",
    "        BLOCK_ROWS=block_rows\n",
    ")\n",
    "\n",
    "\n",
    "kernel = Template('''\n",
    "    extern \"C\" __global__\n",
    "    void execute(\n",
    "        const ${DTYPE} * __restrict__ a,\n",
    "        const ${DTYPE} * __restrict__ b,\n",
    "        const ${DTYPE} * __restrict__ c,\n",
    "        const ${DTYPE} * __restrict__ d,\n",
    "        ${DTYPE} *solution\n",
    "    ){\n",
    "        const unsigned int n = ${SYS_DEPTH};\n",
    "        const unsigned int num_chunks = ${STRIDE};\n",
    "        const unsigned int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        if (idx >= num_chunks) {\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        ${DTYPE} cp[n];\n",
    "        ${DTYPE} dp[n];\n",
    "\n",
    "        cp[0] = c[idx] / b[idx];\n",
    "        dp[0] = d[idx] / b[idx];\n",
    "\n",
    "        #pragma unroll\n",
    "        for (int j = 1; j < n; ++j) {\n",
    "            unsigned int indj = idx+(j*num_chunks);\n",
    "            const ${DTYPE} norm_factor = 1.0 /(b[indj] - a[indj] * cp[j-1]);\n",
    "            cp[j] = c[indj] * norm_factor;\n",
    "            dp[j] = (d[indj] - a[indj] * dp[j-1]) * norm_factor;\n",
    "        }\n",
    "\n",
    "        int stridedIndex = num_chunks*(n-1);\n",
    "        solution[idx + stridedIndex] = dp[n-1];\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int j=n-2; j >= 0; --j)\n",
    "        {\n",
    "            ${DTYPE} s = dp[j] - cp[j] * solution[idx + stridedIndex];\n",
    "            stridedIndex -= num_chunks;\n",
    "            solution[idx + stridedIndex] = s;\n",
    "        }\n",
    "    }\n",
    "    ''').substitute(\n",
    "        DTYPE='double',\n",
    "        SYS_DEPTH=shape[-1],\n",
    "        SIZE=np.product(shape),\n",
    "        STRIDE=shape[0]*shape[1]\n",
    ")\n",
    "transpose4 = cupy.RawKernel(transpose4_kernel, 'transpose4')\n",
    "transpose = cupy.RawKernel(transpose_kernel, 'transpose')\n",
    "tdma_cupy_kernel = cupy.RawKernel(kernel, 'execute')\n",
    "\n",
    "\n",
    "def tdma_cupy(a, b, c, d, o=None, blocksize=256):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "    \n",
    "    if o is None:\n",
    "      a, b, c, d = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "      o = cupy.empty(a.shape, dtype=a.dtype)\n",
    "\n",
    "    a_tmp, b_tmp, c_tmp, d_tmp, o_out = (cupy.empty(s.shape, dtype=s.dtype) for s in (a,b,c,d,a))\n",
    "    \n",
    "    xdim = int((shape[2] + tile_dim -1) / tile_dim)\n",
    "    ydim = int((shape[1]*shape[0] + tile_dim - 1) / tile_dim)\n",
    "    transpose4(\n",
    "        (xdim, ydim, 1),\n",
    "        (tile_dim, block_rows, 1),\n",
    "        (a, b, c, d, a_tmp, b_tmp, c_tmp, d_tmp, int(shape[2]), int(shape[1]*shape[0]), int(np.product(shape)))\n",
    "    )\n",
    "    \n",
    "    tdma_cupy_kernel(\n",
    "        (math.ceil(a.size / a.shape[-1] / blocksize),),\n",
    "        (blocksize,),\n",
    "        (a_tmp, b_tmp, c_tmp, d_tmp, o_out)\n",
    "    )\n",
    "    \n",
    "    transpose(\n",
    "        (ydim, xdim, 1),\n",
    "        (tile_dim, block_rows, 1),\n",
    "        (o_out, o, int(shape[1]*shape[0]), int(shape[2]), int(np.product(shape)))\n",
    "    )\n",
    "    return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_futhark(a, b, c, d, futhark_tke):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    # dim1, dim2, seg_size = a.shape\n",
    "    # # seg_count = dim1*dim2\n",
    "    # a_s = np.reshape(a, (dim1*dim2, seg_size))\n",
    "    # b_s = np.reshape(b, (dim1*dim2, seg_size))\n",
    "    # c_s = np.reshape(c, (dim1*dim2, seg_size))\n",
    "    # d_s = np.reshape(d, (dim1*dim2, seg_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "    res = futhark_tke.tridagNested(a_s, b_s, c_s, d_s)\n",
    "    \n",
    "    \n",
    "    return res #np.reshape(res, a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "a, b, c, d = np.random.randn(4, *shape)\n",
    "res_naive = tdma_naive(a, b, c, d)\n",
    "\n",
    "for imp in (tdma_jax_cuda, tdma_cupy, tdma_cupy_old, tdma_lapack, tdma_numba, tdma_numba_cuda, tdma_jax):\n",
    "    np.random.seed(17)\n",
    "    a, b, c, d = np.random.randn(4, *shape)\n",
    "    \n",
    "    out = imp(a, b, c, d)\n",
    "    try:\n",
    "        out = out.get()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    np.testing.assert_allclose(out, res_naive)\n",
    "    print('✔️')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "a, b, c, d = np.random.randn(4, shape[0], shape[1], shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 ms ± 7.49 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_naive(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 ms ± 1.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_lapack(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9 ms ± 77.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_numba(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 ms ± 3.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax(a, b, c, d).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ac, bc, cc, dc = (nb.cuda.to_device(k) for k in (a, b, c, d))\n",
    "tdma_numba_cuda(ac, bc, cc, dc);  # trigger compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 ms ± 519 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_numba_cuda(ac, bc, cc, dc)\n",
    "numba.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "aj, bj, cj, dj = (jnp.array(k).block_until_ready() for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.3 ms ± 94.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax_cuda(aj, bj, cj, dj).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stream = cupy.cuda.stream.Stream()\n",
    "\n",
    "# with stream:\n",
    "ac, bc, cc, dc = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "o = cupy.empty(ac.shape, dtype=ac.dtype)\n",
    "tdma_cupy(ac, bc, cc, dc,o);  # trigger compilation\n",
    "\n",
    "# stream.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8993964195251465\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "runs = 20\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "for i in range(runs):\n",
    "    ac, bc, cc, dc = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "    o = cupy.empty(ac.shape, dtype=ac.dtype)\n",
    "    cupy.cuda.Stream.null.synchronize()\n",
    "    start_time = time.time()    \n",
    "    tdma_cupy(ac, bc, cc, dc, o)\n",
    "    cupy.cuda.Stream.null.synchronize()\n",
    "    total_time += (time.time() - start_time)\n",
    "    del ac\n",
    "    del bc\n",
    "    del cc\n",
    "    del dc\n",
    "\n",
    "print((total_time) * 1000 / float(runs))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Futhark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1, dim2, seg_size = a.shape\n",
    "# seg_count = dim1*dim2\n",
    "a_s = np.reshape(a, (dim1*dim2, seg_size))\n",
    "b_s = np.reshape(b, (dim1*dim2, seg_size))\n",
    "c_s = np.reshape(c, (dim1*dim2, seg_size))\n",
    "d_s = np.reshape(d, (dim1*dim2, seg_size))\n",
    "\n",
    "futhark_tke = tke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.4 ms ± 743 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_futhark(a, b, c, d, futhark_tke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Jax without transposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_jax_kernel_notrans(a, b, c, d):\n",
    "    def compute_primes(last_primes, x):\n",
    "        last_cp, last_dp = last_primes\n",
    "        a, b, c, d = x\n",
    "\n",
    "        denom = 1. / (b - a * last_cp)\n",
    "        cp = c * denom\n",
    "        dp = (d - a * last_dp) * denom\n",
    "\n",
    "        new_primes = (cp, dp)\n",
    "        return new_primes, new_primes\n",
    "\n",
    "    diags = (a, b, c, d)\n",
    "    init = jnp.zeros((a.shape[1], a.shape[2]))\n",
    "    _, (cp, dp) = jax.lax.scan(compute_primes, (init, init), diags)\n",
    "\n",
    "    def backsubstitution(last_x, x):\n",
    "        cp, dp = x\n",
    "        new_x = dp - cp * last_x\n",
    "        return new_x, new_x\n",
    "\n",
    "    _, sol = jax.lax.scan(backsubstitution, init, (cp[::-1], dp[::-1]))\n",
    "\n",
    "    return sol[::-1]\n",
    "\n",
    "\n",
    "tdma_jax_notrans = jax.jit(tdma_jax_kernel_notrans, backend='cpu')\n",
    "tdma_jax_cuda_notrans = jax.jit(tdma_jax_kernel_notrans, backend='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "at, bt, ct, dt = (k.T for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 ms ± 3.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax_notrans(at, bt, ct, dt).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "aj, bj, cj, dj = (jnp.array(k.T).block_until_ready() for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2 ms ± 453 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax_cuda_notrans(aj, bj, cj, dj).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('pyhpc-bench-gpu': conda)",
   "language": "python",
   "name": "python37864bitpyhpcbenchgpuconda7a23deddf636465b80e8faa969783666"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
