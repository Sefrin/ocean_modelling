{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tridiagonal matrix solver benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "env: OMP_NUM_THREADS=1\nenv: CUDA_VISIBLE_DEVICES=0\n"
    }
   ],
   "source": [
    "# %env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fri Oct  2 15:31:32 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce RTX 207...  Off  | 00000000:09:00.0  On |                  N/A |\n|  0%   38C    P5    14W / 215W |    607MiB /  7974MiB |      7%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1153      G   /usr/lib/xorg/Xorg                 35MiB |\n|    0   N/A  N/A      2647      G   /usr/lib/xorg/Xorg                162MiB |\n|    0   N/A  N/A      2785      G   /usr/bin/gnome-shell              154MiB |\n|    0   N/A  N/A      3549      G   ...AAAAAAAAA= --shared-files      170MiB |\n|    0   N/A  N/A     38883      G   ...AAAAAAAAA= --shared-files       71MiB |\n+-----------------------------------------------------------------------------+\n"
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import numba.cuda\n",
    "from tke import tke\n",
    "from scipy.linalg import lapack\n",
    "from jax_xla import tridiag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (360, 160, 115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement TDMA\n",
    "\n",
    "#### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_naive(a, b, c, d):\n",
    "    \"\"\"\n",
    "    Solves many tridiagonal matrix systems with diagonals a, b, c and RHS vectors d.\n",
    "    \"\"\"\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    n = a.shape[-1]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        w = a[..., i] / b[..., i - 1]\n",
    "        b[..., i] += -w * c[..., i - 1]\n",
    "        d[..., i] += -w * d[..., i - 1]\n",
    "\n",
    "    out = np.empty_like(a)\n",
    "    out[..., -1] = d[..., -1] / b[..., -1]\n",
    "\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        out[..., i] = (d[..., i] - c[..., i] * out[..., i + 1]) / b[..., i]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_lapack(a, b, c, d):\n",
    "    a[..., 0] = c[..., -1] = 0  # remove couplings between slices\n",
    "    return lapack.dgtsv(a.flatten()[1:], b.flatten(), c.flatten()[:-1], d.flatten())[3].reshape(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.guvectorize([(nb.float64[:],) * 5], '(n), (n), (n), (n) -> (n)', nopython=True)\n",
    "def tdma_numba(a, b, c, d, out):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    n = a.shape[0]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        w = a[i] / b[i - 1]\n",
    "        b[i] += -w * c[i - 1]\n",
    "        d[i] += -w * d[i - 1]\n",
    "\n",
    "    out[-1] = d[-1] / b[-1]\n",
    "\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        out[i] = (d[i] - c[i] * out[i + 1]) / b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nconst = shape[-1]\n",
    "\n",
    "\n",
    "@nb.cuda.jit()\n",
    "def tdma_numba_cuda_kernel(a, b, c, d, out):\n",
    "    i, j = nb.cuda.grid(2)\n",
    "    \n",
    "    if not(i < a.shape[0] and j < a.shape[1]):\n",
    "        return\n",
    "\n",
    "    n = a.shape[2]\n",
    "    \n",
    "    cp = nb.cuda.local.array((nconst,), dtype=nb.float64)\n",
    "    dp = nb.cuda.local.array((nconst,), dtype=nb.float64)\n",
    "    \n",
    "    cp[0] = c[i, j, 0] / b[i, j, 0]\n",
    "    dp[0] = d[i, j, 0] / b[i, j, 0]\n",
    "    \n",
    "    for k in range(1, n):\n",
    "        norm_factor = b[i, j, k] - a[i, j, k] * cp[k-1]\n",
    "        cp[k] = c[i, j, k] / norm_factor\n",
    "        dp[k] = (d[i, j, k] - a[i, j, k] * dp[k-1]) / norm_factor\n",
    "\n",
    "    out[i, j, n-1] = dp[n-1]\n",
    "\n",
    "    for k in range(n - 2, -1, -1):\n",
    "        out[i, j, k] = dp[k] - cp[k] * out[i, j, k+1]\n",
    "        \n",
    "\n",
    "def tdma_numba_cuda(a, b, c, d):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    threadsperblock = (16, 16)\n",
    "    blockspergrid_x = math.ceil(a.shape[0] / threadsperblock[0])\n",
    "    blockspergrid_y = math.ceil(a.shape[1] / threadsperblock[1])\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    out = nb.cuda.device_array(a.shape, dtype=a.dtype)\n",
    "    tdma_numba_cuda_kernel[blockspergrid, threadsperblock](a, b, c, d, out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.lax\n",
    "\n",
    "\n",
    "def tdma_jax_kernel(a, b, c, d):\n",
    "    def compute_primes(last_primes, x):\n",
    "        last_cp, last_dp = last_primes\n",
    "        a, b, c, d = x\n",
    "\n",
    "        denom = 1. / (b - a * last_cp)\n",
    "        cp = c * denom\n",
    "        dp = (d - a * last_dp) * denom\n",
    "\n",
    "        new_primes = (cp, dp)\n",
    "        return new_primes, new_primes\n",
    "\n",
    "    diags = (a.T, b.T, c.T, d.T)\n",
    "    init = jnp.zeros((a.shape[1], a.shape[0]))\n",
    "    _, (cp, dp) = jax.lax.scan(compute_primes, (init, init), diags)\n",
    "\n",
    "    def backsubstitution(last_x, x):\n",
    "        cp, dp = x\n",
    "        new_x = dp - cp * last_x\n",
    "        return new_x, new_x\n",
    "\n",
    "    _, sol = jax.lax.scan(backsubstitution, init, (cp[::-1], dp[::-1]))\n",
    "\n",
    "    return sol[::-1].T\n",
    "\n",
    "\n",
    "tdma_jax = jax.jit(tdma_jax_kernel, backend='cpu')\n",
    "tdma_jax_cuda = jax.jit(tridiag.tridiag, backend='gpu') # jax.jit(tdma_jax_kernel, backend='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "kernel_old = Template('''\n",
    "extern \"C\" __global__\n",
    "void execute(\n",
    "    const ${DTYPE} *a,\n",
    "    const ${DTYPE} *b,\n",
    "    const ${DTYPE} *c,\n",
    "    const ${DTYPE} *d,\n",
    "    ${DTYPE} *solution\n",
    "){\n",
    "    const size_t m = ${SYS_DEPTH};\n",
    "    const size_t total_size = ${SIZE};\n",
    "    const size_t idx = (blockIdx.x * blockDim.x + threadIdx.x) * m;\n",
    "\n",
    "    if (idx >= total_size) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    ${DTYPE} cp[${SYS_DEPTH}];\n",
    "    ${DTYPE} dp[${SYS_DEPTH}];\n",
    "\n",
    "    cp[0] = c[idx] / b[idx];\n",
    "    dp[0] = d[idx] / b[idx];\n",
    "\n",
    "    for (ptrdiff_t j = 1; j < m; ++j) {\n",
    "        const ${DTYPE} norm_factor = b[idx+j] - a[idx+j] * cp[j-1];\n",
    "        cp[j] = c[idx+j] / norm_factor;\n",
    "        dp[j] = (d[idx+j] - a[idx+j] * dp[j-1]) / norm_factor;\n",
    "    }\n",
    "\n",
    "    solution[idx + m-1] = dp[m-1];\n",
    "    for (ptrdiff_t j=m-2; j >= 0; --j) {\n",
    "        solution[idx + j] = dp[j] - cp[j] * solution[idx + j+1];\n",
    "    }\n",
    "}\n",
    "''').substitute(\n",
    "    DTYPE='double',\n",
    "    SYS_DEPTH=shape[-1],\n",
    "    SIZE=np.product(shape)\n",
    ")\n",
    "\n",
    "tdma_cupy_kernel_old = cupy.RawKernel(kernel_old, 'execute')\n",
    "\n",
    "\n",
    "def tdma_cupy_old(a, b, c, d,  blocksize=256):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "    a, b, c, d = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "    out = cupy.empty(a.shape, dtype=a.dtype)\n",
    "    \n",
    "    tdma_cupy_kernel_old(\n",
    "        (math.ceil(a.size / a.shape[-1] / blocksize),),\n",
    "        (blocksize,),\n",
    "        (a, b, c, d, out)\n",
    "    )\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_dim = 32\n",
    "block_rows = 8 \n",
    "transpose4_kernel = Template('''\n",
    "extern \"C\" __global__\n",
    "    void transpose4(\n",
    "  const ${DTYPE}* a,\n",
    "  const ${DTYPE}* b,\n",
    "  const ${DTYPE}* c,\n",
    "  const ${DTYPE}* d,\n",
    "  ${DTYPE}* a_t,\n",
    "  ${DTYPE}* b_t,\n",
    "  ${DTYPE}* c_t,\n",
    "  ${DTYPE}* d_t,\n",
    "  int xdim,\n",
    "  int ydim,\n",
    "  int total_size\n",
    ")\n",
    "{\n",
    "  __shared__ ${DTYPE} tile[4*${TILE_DIM}][${TILE_DIM}+1];\n",
    "\n",
    "  int x = blockIdx.x * ${TILE_DIM} + threadIdx.x;\n",
    "  int y = blockIdx.y * ${TILE_DIM} + threadIdx.y;\n",
    "  \n",
    "  if (x < xdim)\n",
    "  {\n",
    "    for (int j = 0; j < ${TILE_DIM}; j += ${BLOCK_ROWS})\n",
    "    {\n",
    "        int index = (y+j)*xdim + x;\n",
    "        if (index < total_size)\n",
    "        {  \n",
    "          tile[threadIdx.y+j][threadIdx.x] = a[index];\n",
    "          tile[${TILE_DIM} + threadIdx.y+j][threadIdx.x] = b[index];\n",
    "          tile[2 * ${TILE_DIM} + threadIdx.y+j][threadIdx.x] = c[index];\n",
    "          tile[3 * ${TILE_DIM} + threadIdx.y+j][threadIdx.x] = d[index];\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  x = blockIdx.y * ${TILE_DIM} + threadIdx.x;  // transpose block offset\n",
    "  y = blockIdx.x * ${TILE_DIM} + threadIdx.y;\n",
    "  if (x < ydim)\n",
    "  {\n",
    "    for (int j = 0; j < ${TILE_DIM}; j += ${BLOCK_ROWS})\n",
    "    {\n",
    "      int index = (y+j)*ydim + x;\n",
    "      if (index < total_size)\n",
    "      {\n",
    "        \n",
    "        a_t[index] = tile[threadIdx.x][threadIdx.y + j];\n",
    "        b_t[index] = tile[${TILE_DIM} + threadIdx.x][threadIdx.y + j];\n",
    "        c_t[index] = tile[2 * ${TILE_DIM} + threadIdx.x][threadIdx.y + j];\n",
    "        d_t[index] = tile[3 * ${TILE_DIM} + threadIdx.x][threadIdx.y + j];\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''').substitute(\n",
    "        DTYPE='double',\n",
    "        TILE_DIM=tile_dim,\n",
    "        BLOCK_ROWS=block_rows\n",
    ")\n",
    "\n",
    "\n",
    "transpose_kernel = Template('''\n",
    "extern \"C\" __global__\n",
    "void transpose(\n",
    "  const ${DTYPE}* m,\n",
    "  ${DTYPE}* m_t,\n",
    "  int xdim,\n",
    "  int ydim,\n",
    "  int total_size\n",
    ")\n",
    "{\n",
    "  __shared__ ${DTYPE} tile[${TILE_DIM}][${TILE_DIM}+1];\n",
    "\n",
    "  int x = blockIdx.x * ${TILE_DIM} + threadIdx.x;\n",
    "  int y = blockIdx.y * ${TILE_DIM} + threadIdx.y;\n",
    "  \n",
    "  if (x < xdim)\n",
    "  {\n",
    "    for (int j = 0; j < ${TILE_DIM}; j += ${BLOCK_ROWS})\n",
    "    {\n",
    "        int index = (y+j)*xdim + x;\n",
    "        if (index < total_size)\n",
    "          tile[threadIdx.y+j][threadIdx.x] = m[index];\n",
    "    }\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  x = blockIdx.y * ${TILE_DIM} + threadIdx.x;  // transpose block offset\n",
    "  y = blockIdx.x * ${TILE_DIM} + threadIdx.y;\n",
    "  if (x < ydim)\n",
    "  {\n",
    "    for (int j = 0; j < ${TILE_DIM}; j += ${BLOCK_ROWS})\n",
    "    {\n",
    "      int index = (y+j)*ydim + x;\n",
    "      if (index < total_size)\n",
    "        m_t[index] = tile[threadIdx.x][threadIdx.y + j];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "''').substitute(\n",
    "        DTYPE='double',\n",
    "        TILE_DIM=tile_dim,\n",
    "        BLOCK_ROWS=block_rows\n",
    ")\n",
    "\n",
    "\n",
    "kernel = Template('''\n",
    "    extern \"C\" __global__\n",
    "    void execute(\n",
    "        const ${DTYPE} * __restrict__ a,\n",
    "        const ${DTYPE} * __restrict__ b,\n",
    "        const ${DTYPE} * __restrict__ c,\n",
    "        const ${DTYPE} * __restrict__ d,\n",
    "        ${DTYPE} *solution\n",
    "    ){\n",
    "        const unsigned int n = ${SYS_DEPTH};\n",
    "        const unsigned int num_chunks = ${STRIDE};\n",
    "        const unsigned int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        if (idx >= num_chunks) {\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        ${DTYPE} cp[n];\n",
    "        ${DTYPE} dp[n];\n",
    "\n",
    "        cp[0] = c[idx] / b[idx];\n",
    "        dp[0] = d[idx] / b[idx];\n",
    "\n",
    "        #pragma unroll\n",
    "        for (int j = 1; j < n; ++j) {\n",
    "            unsigned int indj = idx+(j*num_chunks);\n",
    "            const ${DTYPE} norm_factor = 1.0 /(b[indj] - a[indj] * cp[j-1]);\n",
    "            cp[j] = c[indj] * norm_factor;\n",
    "            dp[j] = (d[indj] - a[indj] * dp[j-1]) * norm_factor;\n",
    "        }\n",
    "\n",
    "        int stridedIndex = num_chunks*(n-1);\n",
    "        solution[idx + stridedIndex] = dp[n-1];\n",
    "        \n",
    "        #pragma unroll\n",
    "        for (int j=n-2; j >= 0; --j)\n",
    "        {\n",
    "            ${DTYPE} s = dp[j] - cp[j] * solution[idx + stridedIndex];\n",
    "            stridedIndex -= num_chunks;\n",
    "            solution[idx + stridedIndex] = s;\n",
    "        }\n",
    "    }\n",
    "    ''').substitute(\n",
    "        DTYPE='double',\n",
    "        SYS_DEPTH=shape[-1],\n",
    "        SIZE=np.product(shape),\n",
    "        STRIDE=shape[0]*shape[1]\n",
    ")\n",
    "transpose4 = cupy.RawKernel(transpose4_kernel, 'transpose4')\n",
    "transpose = cupy.RawKernel(transpose_kernel, 'transpose')\n",
    "tdma_cupy_kernel = cupy.RawKernel(kernel, 'execute')\n",
    "\n",
    "\n",
    "def tdma_cupy(a, b, c, d, o=None, blocksize=256):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "    \n",
    "    if o is None:\n",
    "      a, b, c, d = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "      o = cupy.empty(a.shape, dtype=a.dtype)\n",
    "\n",
    "    a_tmp, b_tmp, c_tmp, d_tmp, o_out = (cupy.empty(s.shape, dtype=s.dtype) for s in (a,b,c,d,a))\n",
    "    \n",
    "    xdim = int((shape[2] + tile_dim -1) / tile_dim)\n",
    "    ydim = int((shape[1]*shape[0] + tile_dim - 1) / tile_dim)\n",
    "    transpose4(\n",
    "        (xdim, ydim, 1),\n",
    "        (tile_dim, block_rows, 1),\n",
    "        (a, b, c, d, a_tmp, b_tmp, c_tmp, d_tmp, int(shape[2]), int(shape[1]*shape[0]), int(np.product(shape)))\n",
    "    )\n",
    "    \n",
    "    tdma_cupy_kernel(\n",
    "        (math.ceil(a.size / a.shape[-1] / blocksize),),\n",
    "        (blocksize,),\n",
    "        (a_tmp, b_tmp, c_tmp, d_tmp, o_out)\n",
    "    )\n",
    "    \n",
    "    transpose(\n",
    "        (ydim, xdim, 1),\n",
    "        (tile_dim, block_rows, 1),\n",
    "        (o_out, o, int(shape[1]*shape[0]), int(shape[2]), int(np.product(shape)))\n",
    "    )\n",
    "    return o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_futhark(a, b, c, d, futhark_tke):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    # dim1, dim2, seg_size = a.shape\n",
    "    # # seg_count = dim1*dim2\n",
    "    # a_s = np.reshape(a, (dim1*dim2, seg_size))\n",
    "    # b_s = np.reshape(b, (dim1*dim2, seg_size))\n",
    "    # c_s = np.reshape(c, (dim1*dim2, seg_size))\n",
    "    # d_s = np.reshape(d, (dim1*dim2, seg_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "    res = futhark_tke.tridagNested(a_s, b_s, c_s, d_s)\n",
    "    \n",
    "    \n",
    "    return res #np.reshape(res, a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'jaxlib.xla_extension.XlaOp'>\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Argument '<jaxlib.xla_extension.XlaOp object at 0x7f67c4784430>' of type '<class 'jaxlib.xla_extension.XlaOp'>' is not a valid JAX type",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d65de973d211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/api.py\u001b[0m in \u001b[0;36mf_jitted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     out = xla.xla_call(flat_fun, *args_flat, device=device, backend=backend,\n\u001b[0;32m--> 171\u001b[0;31m                        name=flat_fun.__name__, donated_invars=donated_invars)\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1121\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtop_trace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnew_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m       \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(fun, device, backend, name, donated_invars, *args)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_xla_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrappedFun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonated_invars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\n\u001b[0;32m--> 527\u001b[0;31m                                *unsafe_map(arg_spec, args))\n\u001b[0m\u001b[1;32m    528\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_stores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_callable\u001b[0;34m(fun, device, backend, name, donated_invars, *arg_specs)\u001b[0m\n\u001b[1;32m    643\u001b[0m   out_nodes = jaxpr_subcomp(\n\u001b[1;32m    644\u001b[0m       \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAxisEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnreps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxla_consts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m       extend_name_stack(wrap_name(name, 'jit')), *xla_args)\n\u001b[0m\u001b[1;32m    646\u001b[0m   \u001b[0mout_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m   \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mjaxpr_subcomp\u001b[0;34m(c, jaxpr, backend, axis_env, consts, name_stack, *args)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackend_specific_translations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0mrule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend_specific_translations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/ocean_modelling/tridiag/jax_xla/tridiag.py\u001b[0m in \u001b[0;36m_tridiag_gpu_translation_rule\u001b[0;34m(computation_builder, a, b, c, d)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_tridiag_gpu_translation_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputation_builder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcuda_tridiag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtridiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputation_builder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtridiag_abstract_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/ocean_modelling/tridiag/jax_xla/cuda_tridiag.py\u001b[0m in \u001b[0;36mtridiag\u001b[0;34m(builder, a, b, c, d)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# print(type(transpose(a, (0,1,2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0ma_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mb_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/lax/lax.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(operand, permutation)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moperand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranspose_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m def argmin(operand: Array, axis: int,\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mtop_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtop_trace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/lax/lax.py\u001b[0m in \u001b[0;36m_transpose_impl\u001b[0;34m(operand, permutation)\u001b[0m\n\u001b[1;32m   3381\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeviceArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranspose_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_transpose_shape_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36marg_spec\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m   \u001b[0maval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabstractify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mabstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0maval_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytype_aval_mappings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maval_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0maval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Argument '{x}' of type '{type(x)}' is not a valid JAX type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_make_abstract_python_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument '<jaxlib.xla_extension.XlaOp object at 0x7f67c4784430>' of type '<class 'jaxlib.xla_extension.XlaOp'>' is not a valid JAX type"
     ]
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "a, b, c, d = np.random.randn(4, *shape)\n",
    "res_naive = tdma_naive(a, b, c, d)\n",
    "\n",
    "for imp in (tdma_jax_cuda, tdma_cupy, tdma_cupy_old, tdma_lapack, tdma_numba, tdma_numba_cuda, tdma_jax):\n",
    "    np.random.seed(17)\n",
    "    a, b, c, d = np.random.randn(4, *shape)\n",
    "    \n",
    "    out = imp(a, b, c, d)\n",
    "    try:\n",
    "        out = out.get()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    np.testing.assert_allclose(out, res_naive)\n",
    "    print('✔️')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "a, b, c, d = np.random.randn(4, shape[0], shape[1], shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "180 ms ± 7.49 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_naive(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "153 ms ± 1.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_lapack(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "99.9 ms ± 77.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_numba(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "119 ms ± 3.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax(a, b, c, d).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ac, bc, cc, dc = (nb.cuda.to_device(k) for k in (a, b, c, d))\n",
    "tdma_numba_cuda(ac, bc, cc, dc);  # trigger compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "17 ms ± 519 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_numba_cuda(ac, bc, cc, dc)\n",
    "numba.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "aj, bj, cj, dj = (jnp.array(k).block_until_ready() for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "17.3 ms ± 94.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax_cuda(aj, bj, cj, dj).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stream = cupy.cuda.stream.Stream()\n",
    "\n",
    "# with stream:\n",
    "ac, bc, cc, dc = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "o = cupy.empty(ac.shape, dtype=ac.dtype)\n",
    "tdma_cupy(ac, bc, cc, dc,o);  # trigger compilation\n",
    "\n",
    "# stream.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.8993964195251465\n"
    }
   ],
   "source": [
    "import time\n",
    "runs = 20\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "for i in range(runs):\n",
    "    ac, bc, cc, dc = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "    o = cupy.empty(ac.shape, dtype=ac.dtype)\n",
    "    cupy.cuda.Stream.null.synchronize()\n",
    "    start_time = time.time()    \n",
    "    tdma_cupy(ac, bc, cc, dc, o)\n",
    "    cupy.cuda.Stream.null.synchronize()\n",
    "    total_time += (time.time() - start_time)\n",
    "    del ac\n",
    "    del bc\n",
    "    del cc\n",
    "    del dc\n",
    "\n",
    "print((total_time) * 1000 / float(runs))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "source": [
    "### Futhark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1, dim2, seg_size = a.shape\n",
    "# seg_count = dim1*dim2\n",
    "a_s = np.reshape(a, (dim1*dim2, seg_size))\n",
    "b_s = np.reshape(b, (dim1*dim2, seg_size))\n",
    "c_s = np.reshape(c, (dim1*dim2, seg_size))\n",
    "d_s = np.reshape(d, (dim1*dim2, seg_size))\n",
    "\n",
    "futhark_tke = tke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "57.4 ms ± 743 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_futhark(a, b, c, d, futhark_tke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Jax without transposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_jax_kernel_notrans(a, b, c, d):\n",
    "    def compute_primes(last_primes, x):\n",
    "        last_cp, last_dp = last_primes\n",
    "        a, b, c, d = x\n",
    "\n",
    "        denom = 1. / (b - a * last_cp)\n",
    "        cp = c * denom\n",
    "        dp = (d - a * last_dp) * denom\n",
    "\n",
    "        new_primes = (cp, dp)\n",
    "        return new_primes, new_primes\n",
    "\n",
    "    diags = (a, b, c, d)\n",
    "    init = jnp.zeros((a.shape[1], a.shape[2]))\n",
    "    _, (cp, dp) = jax.lax.scan(compute_primes, (init, init), diags)\n",
    "\n",
    "    def backsubstitution(last_x, x):\n",
    "        cp, dp = x\n",
    "        new_x = dp - cp * last_x\n",
    "        return new_x, new_x\n",
    "\n",
    "    _, sol = jax.lax.scan(backsubstitution, init, (cp[::-1], dp[::-1]))\n",
    "\n",
    "    return sol[::-1]\n",
    "\n",
    "\n",
    "tdma_jax_notrans = jax.jit(tdma_jax_kernel_notrans, backend='cpu')\n",
    "tdma_jax_cuda_notrans = jax.jit(tdma_jax_kernel_notrans, backend='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "at, bt, ct, dt = (k.T for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "175 ms ± 3.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax_notrans(at, bt, ct, dt).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "aj, bj, cj, dj = (jnp.array(k.T).block_until_ready() for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6.2 ms ± 453 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax_cuda_notrans(aj, bj, cj, dj).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('pyhpc-bench-gpu': conda)",
   "language": "python",
   "name": "python37864bitpyhpcbenchgpuconda7a23deddf636465b80e8faa969783666"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}