{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tridiagonal matrix solver benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "env: OMP_NUM_THREADS=1\nenv: CUDA_VISIBLE_DEVICES=0\n"
    }
   ],
   "source": [
    "# %env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sun Sep 13 21:41:30 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce RTX 207...  Off  | 00000000:09:00.0  On |                  N/A |\n|  0%   46C    P5    21W / 215W |    402MiB /  7974MiB |      5%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1330      G   /usr/lib/xorg/Xorg                 35MiB |\n|    0   N/A  N/A      2334      G   /usr/lib/xorg/Xorg                144MiB |\n|    0   N/A  N/A      2540      G   /usr/bin/gnome-shell              120MiB |\n|    0   N/A  N/A      4833      G   /usr/lib/firefox/firefox            2MiB |\n|    0   N/A  N/A     33280      G   /usr/lib/firefox/firefox            2MiB |\n|    0   N/A  N/A     40879      G   ...AAAAAAAAA= --shared-files       83MiB |\n+-----------------------------------------------------------------------------+\n"
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Load libllvmlite.so .. os.environ[PATH] is: /home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/till/anaconda3/envs/pyhpc-bench-gpu/bin:/home/till/anaconda3/condabin:/usr/local/cuda-11.0/bin:/home/till/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\nChecking /home/till/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/llvmlite/binding/libllvmlite.so\nload_library_permanently(libsvml.so)\nload_library_permanently .. so.environ[PATH] is: /home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/till/anaconda3/envs/pyhpc-bench-gpu/bin:/home/till/anaconda3/condabin:/usr/local/cuda-11.0/bin:/home/till/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\nload_library_permanently(libsvml.so)\n"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import numba.cuda\n",
    "from tke import tke\n",
    "from scipy.linalg import lapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (360, 160, 115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement TDMA\n",
    "\n",
    "#### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_naive(a, b, c, d):\n",
    "    \"\"\"\n",
    "    Solves many tridiagonal matrix systems with diagonals a, b, c and RHS vectors d.\n",
    "    \"\"\"\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    n = a.shape[-1]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        w = a[..., i] / b[..., i - 1]\n",
    "        b[..., i] += -w * c[..., i - 1]\n",
    "        d[..., i] += -w * d[..., i - 1]\n",
    "\n",
    "    out = np.empty_like(a)\n",
    "    out[..., -1] = d[..., -1] / b[..., -1]\n",
    "\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        out[..., i] = (d[..., i] - c[..., i] * out[..., i + 1]) / b[..., i]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_lapack(a, b, c, d):\n",
    "    a[..., 0] = c[..., -1] = 0  # remove couplings between slices\n",
    "    return lapack.dgtsv(a.flatten()[1:], b.flatten(), c.flatten()[:-1], d.flatten())[3].reshape(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.guvectorize([(nb.float64[:],) * 5], '(n), (n), (n), (n) -> (n)', nopython=True)\n",
    "def tdma_numba(a, b, c, d, out):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    n = a.shape[0]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        w = a[i] / b[i - 1]\n",
    "        b[i] += -w * c[i - 1]\n",
    "        d[i] += -w * d[i - 1]\n",
    "\n",
    "    out[-1] = d[-1] / b[-1]\n",
    "\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        out[i] = (d[i] - c[i] * out[i + 1]) / b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nconst = shape[-1]\n",
    "\n",
    "\n",
    "@nb.cuda.jit()\n",
    "def tdma_numba_cuda_kernel(a, b, c, d, out):\n",
    "    i, j = nb.cuda.grid(2)\n",
    "    \n",
    "    if not(i < a.shape[0] and j < a.shape[1]):\n",
    "        return\n",
    "\n",
    "    n = a.shape[2]\n",
    "    \n",
    "    cp = nb.cuda.local.array((nconst,), dtype=nb.float64)\n",
    "    dp = nb.cuda.local.array((nconst,), dtype=nb.float64)\n",
    "    \n",
    "    cp[0] = c[i, j, 0] / b[i, j, 0]\n",
    "    dp[0] = d[i, j, 0] / b[i, j, 0]\n",
    "    \n",
    "    for k in range(1, n):\n",
    "        norm_factor = b[i, j, k] - a[i, j, k] * cp[k-1]\n",
    "        cp[k] = c[i, j, k] / norm_factor\n",
    "        dp[k] = (d[i, j, k] - a[i, j, k] * dp[k-1]) / norm_factor\n",
    "\n",
    "    out[i, j, n-1] = dp[n-1]\n",
    "\n",
    "    for k in range(n - 2, -1, -1):\n",
    "        out[i, j, k] = dp[k] - cp[k] * out[i, j, k+1]\n",
    "        \n",
    "\n",
    "def tdma_numba_cuda(a, b, c, d):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    threadsperblock = (16, 16)\n",
    "    blockspergrid_x = math.ceil(a.shape[0] / threadsperblock[0])\n",
    "    blockspergrid_y = math.ceil(a.shape[1] / threadsperblock[1])\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    out = nb.cuda.device_array(a.shape, dtype=a.dtype)\n",
    "    tdma_numba_cuda_kernel[blockspergrid, threadsperblock](a, b, c, d, out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.lax\n",
    "\n",
    "\n",
    "def tdma_jax_kernel(a, b, c, d):\n",
    "    def compute_primes(last_primes, x):\n",
    "        last_cp, last_dp = last_primes\n",
    "        a, b, c, d = x\n",
    "\n",
    "        denom = 1. / (b - a * last_cp)\n",
    "        cp = c * denom\n",
    "        dp = (d - a * last_dp) * denom\n",
    "\n",
    "        new_primes = (cp, dp)\n",
    "        return new_primes, new_primes\n",
    "\n",
    "    diags = (a.T, b.T, c.T, d.T)\n",
    "    init = jnp.zeros((a.shape[1], a.shape[0]))\n",
    "    _, (cp, dp) = jax.lax.scan(compute_primes, (init, init), diags)\n",
    "\n",
    "    def backsubstitution(last_x, x):\n",
    "        cp, dp = x\n",
    "        new_x = dp - cp * last_x\n",
    "        return new_x, new_x\n",
    "\n",
    "    _, sol = jax.lax.scan(backsubstitution, init, (cp[::-1], dp[::-1]))\n",
    "\n",
    "    return sol[::-1].T\n",
    "\n",
    "\n",
    "tdma_jax = jax.jit(tdma_jax_kernel, backend='cpu')\n",
    "tdma_jax_cuda = jax.jit(tdma_jax_kernel, backend='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "kernel_old = Template('''\n",
    "extern \"C\" __global__\n",
    "void execute(\n",
    "    const ${DTYPE} *a,\n",
    "    const ${DTYPE} *b,\n",
    "    const ${DTYPE} *c,\n",
    "    const ${DTYPE} *d,\n",
    "    ${DTYPE} *solution\n",
    "){\n",
    "    const size_t m = ${SYS_DEPTH};\n",
    "    const size_t total_size = ${SIZE};\n",
    "    const size_t idx = (blockIdx.x * blockDim.x + threadIdx.x) * m;\n",
    "\n",
    "    if (idx >= total_size) {\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    ${DTYPE} cp[${SYS_DEPTH}];\n",
    "    ${DTYPE} dp[${SYS_DEPTH}];\n",
    "\n",
    "    cp[0] = c[idx] / b[idx];\n",
    "    dp[0] = d[idx] / b[idx];\n",
    "\n",
    "    for (ptrdiff_t j = 1; j < m; ++j) {\n",
    "        const ${DTYPE} norm_factor = b[idx+j] - a[idx+j] * cp[j-1];\n",
    "        cp[j] = c[idx+j] / norm_factor;\n",
    "        dp[j] = (d[idx+j] - a[idx+j] * dp[j-1]) / norm_factor;\n",
    "    }\n",
    "\n",
    "    solution[idx + m-1] = dp[m-1];\n",
    "    for (ptrdiff_t j=m-2; j >= 0; --j) {\n",
    "        solution[idx + j] = dp[j] - cp[j] * solution[idx + j+1];\n",
    "    }\n",
    "}\n",
    "''').substitute(\n",
    "    DTYPE='double',\n",
    "    SYS_DEPTH=shape[-1],\n",
    "    SIZE=np.product(shape)\n",
    ")\n",
    "\n",
    "tdma_cupy_kernel_old = cupy.RawKernel(kernel_old, 'execute')\n",
    "\n",
    "\n",
    "def tdma_cupy_old(a, b, c, d, out,  blocksize=256):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "    # a, b, c, d = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "    # out = cupy.empty(a.shape, dtype=a.dtype)\n",
    "    \n",
    "    tdma_cupy_kernel_old(\n",
    "        (math.ceil(a.size / a.shape[-1] / blocksize),),\n",
    "        (blocksize,),\n",
    "        (a, b, c, d, out)\n",
    "    )\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_kernel = Template('''\n",
    "extern \"C\" __global__\n",
    "    void order(\n",
    "        const ${DTYPE} * __restrict__ a,\n",
    "        const ${DTYPE} * __restrict__ b,\n",
    "        const ${DTYPE} * __restrict__ c,\n",
    "        const ${DTYPE} * __restrict__ d,\n",
    "        ${DTYPE} *a_out,\n",
    "        ${DTYPE} *b_out,\n",
    "        ${DTYPE} *c_out,\n",
    "        ${DTYPE} *d_out\n",
    "    ){\n",
    "        \n",
    "        const unsigned int source_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "        if (source_idx >= ${SIZE})\n",
    "            return;\n",
    "\n",
    "        const unsigned int m = ${SYS_DEPTH};\n",
    "        const unsigned int stride = ${STRIDE};\n",
    "        const unsigned int seg_idx = source_idx / m;\n",
    "        const unsigned int seg_offset = source_idx % m;\n",
    "        const unsigned int target_idx = seg_offset * stride + seg_idx;\n",
    "\n",
    "        a_out[target_idx] = a[source_idx]; \n",
    "        b_out[target_idx] = b[source_idx];\n",
    "        c_out[target_idx] = c[source_idx];\n",
    "        d_out[target_idx] = d[source_idx];\n",
    "    }\n",
    "\n",
    "''').substitute(\n",
    "        DTYPE='double',\n",
    "        SYS_DEPTH=shape[-1],\n",
    "        SIZE=np.product(shape),\n",
    "        STRIDE=shape[0]*shape[1]\n",
    ")\n",
    "\n",
    "order_back_kernel = Template('''\n",
    "extern \"C\" __global__\n",
    "    void order_back(\n",
    "        const ${DTYPE} * __restrict__ out,\n",
    "        ${DTYPE} *o_out\n",
    "    ){\n",
    "\n",
    "        const unsigned int target_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        \n",
    "        if (target_idx >= ${SIZE})\n",
    "            return;\n",
    "        const unsigned int m = ${SYS_DEPTH};\n",
    "        const unsigned int stride = ${STRIDE};\n",
    "        const unsigned int seg_idx = target_idx / m;\n",
    "        const unsigned int seg_offset = target_idx % m;\n",
    "        const unsigned int source_idx = seg_offset * stride + seg_idx;\n",
    "        o_out[target_idx] = out[source_idx];\n",
    "    }\n",
    "\n",
    "''').substitute(\n",
    "        DTYPE='double',\n",
    "        SYS_DEPTH=shape[-1],\n",
    "        SIZE=np.product(shape),\n",
    "        STRIDE=shape[0]*shape[1]\n",
    ")\n",
    "\n",
    "kernel = Template('''\n",
    "    extern \"C\" __global__\n",
    "    void execute(\n",
    "        const ${DTYPE} * __restrict__ a,\n",
    "        const ${DTYPE} * __restrict__ b,\n",
    "        const ${DTYPE} * __restrict__ c,\n",
    "        const ${DTYPE} * __restrict__ d,\n",
    "        ${DTYPE} *solution\n",
    "    ){\n",
    "        const unsigned int m = ${SYS_DEPTH};\n",
    "        const unsigned int total_size = ${SIZE};\n",
    "        const unsigned int stride = total_size / m;\n",
    "        const unsigned int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        if (idx >= stride) {\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        ${DTYPE} cp[${SYS_DEPTH}];\n",
    "        ${DTYPE} dp[${SYS_DEPTH}];\n",
    "\n",
    "        cp[0] = c[idx] / b[idx];\n",
    "        dp[0] = d[idx] / b[idx];\n",
    " \n",
    "        for (int j = 1; j < m; ++j) {\n",
    "            unsigned int indj = idx+(j*stride);\n",
    "            const ${DTYPE} norm_factor = (b[indj] - a[indj] * cp[j-1]);\n",
    "            cp[j] = c[indj] / norm_factor;\n",
    "            dp[j] = (d[indj] - a[indj] * dp[j-1]) / norm_factor;\n",
    "        }\n",
    "\n",
    "        solution[idx + stride*(m-1)] = dp[m-1];\n",
    "\n",
    "        for (int j=m-2; j >= 0; --j) {\n",
    "            solution[idx + stride*j] = dp[j] - cp[j] * solution[idx + stride*(j+1)];\n",
    "        }\n",
    "    }\n",
    "    ''').substitute(\n",
    "        DTYPE='double',\n",
    "        SYS_DEPTH=shape[-1],\n",
    "        SIZE=np.product(shape),\n",
    "        STRIDE=shape[0]*shape[1]\n",
    ")\n",
    "tdma_cupy_kernel = cupy.RawKernel(kernel, 'execute')\n",
    "cupy_order_kernel = cupy.RawKernel(order_kernel, 'order')\n",
    "cupy_order_back_kernel = cupy.RawKernel(order_back_kernel, 'order_back')\n",
    "\n",
    "\n",
    "def tdma_cupy(a, b, c, d, o, blocksize=256):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    # a, b, c, d = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "    \n",
    "    a_tmp, b_tmp, c_tmp, d_tmp, o_out = (cupy.empty(s.shape, dtype=s.dtype) for s in (a,b,c,d,a))\n",
    "\n",
    "    cupy_order_kernel(\n",
    "        (math.ceil(a.size  / blocksize),),\n",
    "        (blocksize,),\n",
    "        (a, b, c, d, a_tmp, b_tmp, c_tmp, d_tmp)\n",
    "    )\n",
    "\n",
    "    tdma_cupy_kernel(\n",
    "        (math.ceil(a.size / a.shape[-1] / blocksize),),\n",
    "        (blocksize,),\n",
    "        (a_tmp, b_tmp, c_tmp, d_tmp, o_out)\n",
    "    )\n",
    "    \n",
    "    cupy_order_back_kernel(\n",
    "        (math.ceil(a.size / blocksize),),\n",
    "        (blocksize,),\n",
    "        (o_out, o)\n",
    "    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_futhark(a, b, c, d, futhark_tke):\n",
    "    assert a.shape == b.shape and a.shape == c.shape and a.shape == d.shape\n",
    "\n",
    "    # dim1, dim2, seg_size = a.shape\n",
    "    # # seg_count = dim1*dim2\n",
    "    # a_s = np.reshape(a, (dim1*dim2, seg_size))\n",
    "    # b_s = np.reshape(b, (dim1*dim2, seg_size))\n",
    "    # c_s = np.reshape(c, (dim1*dim2, seg_size))\n",
    "    # d_s = np.reshape(d, (dim1*dim2, seg_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "    res = futhark_tke.tridagNested(a_s, b_s, c_s, d_s)\n",
    "    \n",
    "    \n",
    "    return res #np.reshape(res, a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "✔️\n✔️\n✔️\n✔️\n✔️\n✔️\n✔️\n"
    }
   ],
   "source": [
    "np.random.seed(17)\n",
    "a, b, c, d = np.random.randn(4, *shape)\n",
    "res_naive = tdma_naive(a, b, c, d)\n",
    "\n",
    "for imp in (tdma_cupy, tdma_cupy_old, tdma_lapack, tdma_numba, tdma_numba_cuda, tdma_jax, tdma_jax_cuda):\n",
    "    np.random.seed(17)\n",
    "    a, b, c, d = np.random.randn(4, *shape)\n",
    "    out = imp(a, b, c, d)\n",
    "    try:\n",
    "        out = out.get()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    np.testing.assert_allclose(out, res_naive)\n",
    "    print('✔️')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)\n",
    "a, b, c, d = np.random.randn(4, shape[0], shape[1], shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "89.1 ms ± 508 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_naive(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6.19 µs ± 37.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_lapack(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1 s ± 10.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_numba(a, b, c, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.15 s ± 13.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax(a, b, c, d).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "ERROR:numba.cuda.cudadrv.driver:Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY\nERROR:numba.cuda.cudadrv.driver:Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY\n"
    },
    {
     "output_type": "error",
     "ename": "CudaAPIError",
     "evalue": "[2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_attempt_allocation\u001b[0;34m(self, allocator)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCudaAPIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mallocator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_cuda_api_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_check_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCudaDriverError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA initialized before forking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCudaAPIError\u001b[0m: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-47c9d6874d2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtdma_numba_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0;31m# trigger compilation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-47c9d6874d2b>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtdma_numba_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0;31m# trigger compilation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36m_require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/api.py\u001b[0m in \u001b[0;36mto_device\u001b[0;34m(obj, stream, copy, to)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevicearray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36mauto_device\u001b[0;34m(obj, stream, copy)\u001b[0m\n\u001b[1;32m    764\u001b[0m                 subok=True)\n\u001b[1;32m    765\u001b[0m             \u001b[0msentry_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mdevobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_array_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mdevobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36mfrom_array_like\u001b[0;34m(ary, stream, gpu_data)\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;34m\"Create a DeviceNDArray object that is like ary.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     return DeviceNDArray(ary.shape, ary.strides, ary.dtype,\n\u001b[0;32m--> 688\u001b[0;31m                          writeback=ary, stream=stream, gpu_data=gpu_data)\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, strides, dtype, stream, writeback, gpu_data)\u001b[0m\n\u001b[1;32m    102\u001b[0m                                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                                                 self.dtype.itemsize)\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mgpu_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_memory_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mmemalloc\u001b[0;34m(self, bytesize)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemhostalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mportable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mmemalloc\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attempt_allocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_alloc_finalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_attempt_allocation\u001b[0;34m(self, allocator)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeallocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;31m# try again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mallocator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attempt_allocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'call driver api: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_cuda_api_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyhpc-bench-gpu/lib/python3.7/site-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_check_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_getpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCudaDriverError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA initialized before forking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCudaAPIError\u001b[0m: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY"
     ]
    }
   ],
   "source": [
    "ac, bc, cc, dc = (nb.cuda.to_device(k) for k in (a, b, c, d))\n",
    "tdma_numba_cuda(ac, bc, cc, dc);  # trigger compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tdma_numba_cuda(ac, bc, cc, dc)\n",
    "numba.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aj, bj, cj, dj = (jnp.array(k).block_until_ready() for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tdma_jax_cuda(aj, bj, cj, dj).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e102368cb743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# with stream:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtdma_cupy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0;31m# trigger compilation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# stream = cupy.cuda.stream.Stream()\n",
    "\n",
    "# with stream:\n",
    "ac, bc, cc, dc = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "o = cupy.empty(ac.shape, dtype=ac.dtype)\n",
    "tdma_cupy(ac, bc, cc, dc);  # trigger compilation\n",
    "\n",
    "# stream.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6e23699491d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "runs = 20\n",
    "\n",
    "total_time = 0\n",
    "\n",
    "for i in range(runs):\n",
    "    ac, bc, cc, dc = (cupy.asarray(k) for k in (a, b, c, d))\n",
    "    o = cupy.empty(ac.shape, dtype=ac.dtype)    \n",
    "\n",
    "    cupy.cuda.Stream.null.synchronize()\n",
    "    start_time = time.time()    \n",
    "    tdma_cupy(ac, bc, cc, dc, o)\n",
    "    cupy.cuda.Stream.null.synchronize()\n",
    "    total_time += (time.time() - start_time)\n",
    "    del ac\n",
    "    del bc\n",
    "    del cc\n",
    "    del dc\n",
    "\n",
    "print((total_time) * 1000 / float(runs))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "source": [
    "### Futhark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1, dim2, seg_size = a.shape\n",
    "# seg_count = dim1*dim2\n",
    "a_s = np.reshape(a, (dim1*dim2, seg_size))\n",
    "b_s = np.reshape(b, (dim1*dim2, seg_size))\n",
    "c_s = np.reshape(c, (dim1*dim2, seg_size))\n",
    "d_s = np.reshape(d, (dim1*dim2, seg_size))\n",
    "\n",
    "futhark_tke = tke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "857 µs ± 3.02 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_futhark(a, b, c, d, futhark_tke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Jax without transposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdma_jax_kernel_notrans(a, b, c, d):\n",
    "    def compute_primes(last_primes, x):\n",
    "        last_cp, last_dp = last_primes\n",
    "        a, b, c, d = x\n",
    "\n",
    "        denom = 1. / (b - a * last_cp)\n",
    "        cp = c * denom\n",
    "        dp = (d - a * last_dp) * denom\n",
    "\n",
    "        new_primes = (cp, dp)\n",
    "        return new_primes, new_primes\n",
    "\n",
    "    diags = (a, b, c, d)\n",
    "    init = jnp.zeros((a.shape[1], a.shape[2]))\n",
    "    _, (cp, dp) = jax.lax.scan(compute_primes, (init, init), diags)\n",
    "\n",
    "    def backsubstitution(last_x, x):\n",
    "        cp, dp = x\n",
    "        new_x = dp - cp * last_x\n",
    "        return new_x, new_x\n",
    "\n",
    "    _, sol = jax.lax.scan(backsubstitution, init, (cp[::-1], dp[::-1]))\n",
    "\n",
    "    return sol[::-1]\n",
    "\n",
    "\n",
    "tdma_jax_notrans = jax.jit(tdma_jax_kernel_notrans, backend='cpu')\n",
    "tdma_jax_cuda_notrans = jax.jit(tdma_jax_kernel_notrans, backend='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "at, bt, ct, dt = (k.T for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537 ms ± 3.31 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax_notrans(at, bt, ct, dt).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "aj, bj, cj, dj = (jnp.array(k.T).block_until_ready() for k in (a, b, c, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.1 ms ± 942 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tdma_jax_cuda_notrans(aj, bj, cj, dj).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('pyhpc-bench-gpu': conda)",
   "language": "python",
   "name": "python37864bitpyhpcbenchgpuconda7a23deddf636465b80e8faa969783666"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}